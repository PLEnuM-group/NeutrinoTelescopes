{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jammy_flows\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, IterableDataset, random_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDF5PhotonTable(IterableDataset):\n",
    "    def __init__(self, filename, start, end):\n",
    "        super(HDF5PhotonTable).__init__()\n",
    "        assert end > start, \"end >= start\"\n",
    "        self._hdl = h5py.File(filename)\n",
    "        self.size = len(self._hdl[\"photon_tables\"].keys())\n",
    "        self.start = start\n",
    "        self.end = min((self.size, end)) \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:  # single-process data loading, return the full iterator\n",
    "            iter_start = self.start\n",
    "            iter_end = self.end\n",
    "        else:  # in a worker process\n",
    "            # split workload\n",
    "            per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))\n",
    "            worker_id = worker_info.id\n",
    "            iter_start = self.start + worker_id * per_worker\n",
    "            iter_end = min(iter_start + per_worker, self.end)\n",
    "\n",
    "        ds_groups = sorted(self._hdl[\"photon_tables\"].keys())\n",
    "\n",
    "        for i in range(iter_start, iter_end):\n",
    "            grp = ds_groups[i]\n",
    "            yield dict(self._hdl[\"photon_tables\"][grp].attrs), self._hdl[\"photon_tables\"][grp][:]\n",
    "       \n",
    "    def __del__(self):\n",
    "        if hasattr(self, \"_hdl\"):\n",
    "            self._hdl.close()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'HDF5PhotonTable' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/chrhck/.julia/dev/NeutrinoTelescopes/notebooks/fit_normalizing_flow.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/chrhck/.julia/dev/NeutrinoTelescopes/notebooks/fit_normalizing_flow.ipynb#ch0000008?line=0'>1</a>\u001b[0m dataset \u001b[39m=\u001b[39m HDF5PhotonTable(\u001b[39m\"\u001b[39m\u001b[39m../assets/photon_table.hd5\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1000\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/chrhck/.julia/dev/NeutrinoTelescopes/notebooks/fit_normalizing_flow.ipynb#ch0000008?line=1'>2</a>\u001b[0m rng \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mGenerator()\u001b[39m.\u001b[39mmanual_seed(\u001b[39m31337\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/chrhck/.julia/dev/NeutrinoTelescopes/notebooks/fit_normalizing_flow.ipynb#ch0000008?line=3'>4</a>\u001b[0m random_split(dataset, [\u001b[39m3\u001b[39;49m, \u001b[39m7\u001b[39;49m], generator\u001b[39m=\u001b[39;49mrng)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:310\u001b[0m, in \u001b[0;36mrandom_split\u001b[0;34m(dataset, lengths, generator)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[39mRandomly split a dataset into non-overlapping new datasets of given lengths.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[39mOptionally fix the generator for reproducible results, e.g.:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39m    generator (Generator): Generator used for the random permutation.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[39m# Cannot verify that dataset is Sized\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39msum\u001b[39m(lengths) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39;49m(dataset):    \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSum of input lengths does not equal the length of the input dataset!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    313\u001b[0m indices \u001b[39m=\u001b[39m randperm(\u001b[39msum\u001b[39m(lengths), generator\u001b[39m=\u001b[39mgenerator)\u001b[39m.\u001b[39mtolist()\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'HDF5PhotonTable' has no len()"
     ]
    }
   ],
   "source": [
    "dataset = HDF5PhotonTable(\"../assets/photon_table.hd5\", 2, 1000)\n",
    "rng = torch.Generator().manual_seed(31337)\n",
    "\n",
    "random_split(dataset, [3, 7], generator=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'dir_phi': 2.3252314699301686, 'dir_theta': 1.1891255369741582, 'distance': 13.915788650512695, 'energy': 100000.0, 'pos_phi': -1.9184931594632901, 'pos_theta': 1.8274022498946725}, array([[ 3.      ,  3.      ,  4.      , ..., 16.      , 16.      ,\n",
      "        16.      ],\n",
      "       [83.537895, 79.127754, 68.75639 , ..., 71.14623 , 66.41417 ,\n",
      "        68.14038 ]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "for d in :\n",
    "    print(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf=jammy_flows.pdf(\"e1\", \"gggg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['dataset_1', 'dataset_10', 'dataset_11', 'dataset_12', 'dataset_2', 'dataset_3', 'dataset_4', 'dataset_5', 'dataset_6', 'dataset_7', 'dataset_8', 'dataset_9']>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=10\n",
    "\n",
    "for ind in range(num_batches_per_epoch):\n",
    "    this_label_batch=target_samples[ind:ind+batch_size]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    log_pdf,_,_=pdf(this_label_batch)\n",
    "\n",
    "    neg_log_loss=-log_pdf.mean()\n",
    "\n",
    "    neg_log_loss.backward()\n",
    "\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

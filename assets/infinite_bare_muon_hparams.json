{
    "K": 12,
    "epochs": 100,
    "lr": 0.002,
    "mlp_layer_size": 768,
    "mlp_layers": 2,
    "dropout": 0.1,
    "non_linearity": "relu",
    "batch_size": 30000,
    "seed": 1,
    "l2_norm_alpha": 0,
    "adam_beta_1": 0.9,
    "adam_beta_2": 0.999
}